{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to controlHub & Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"./libs\")\n",
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import warnings\n",
    "from os import name\n",
    "from random import *\n",
    "from threading import Condition\n",
    "\n",
    "import jobCtrl as jc\n",
    "import streamsets\n",
    "import streamsets.sdk\n",
    "import utilslib as utl\n",
    "from streamsets.sdk import *\n",
    "from streamsets.sdk.config import *\n",
    "from streamsets.sdk.constants import *\n",
    "from streamsets.sdk.examples import *\n",
    "from streamsets.sdk.exceptions import *\n",
    "from streamsets.sdk.models import *\n",
    "from streamsets.sdk.sch import *\n",
    "from streamsets.sdk.sch_api import *\n",
    "from streamsets.sdk.sch_models import *\n",
    "from streamsets.sdk.sdc import *\n",
    "from streamsets.sdk.sdc_models import *\n",
    "from streamsets.sdk.utils import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "cfg = utl.Config(\"./config_file_streamsets_sch_trailer.yaml\")\n",
    "cfg.load()\n",
    "# ------------------------------------------------\n",
    "os.environ[\"STREAMSETS_SDK_ACTIVATION_KEY\"] = cfg.getSdkKey()\n",
    "_BASE_URL_ = cfg.getUrlSch()\n",
    "print(_BASE_URL_)\n",
    "_SDC_URL_ = cfg.getUrlSdc()\n",
    "print(_SDC_URL_)\n",
    "_TX_URL_ = cfg.getUrlTx()\n",
    "print(_TX_URL_)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ------------------------------------------------\n",
    "# ------------------------------------------------\n",
    "\n",
    "ControlHub.VERIFY_SSL_CERTIFICATES = False\n",
    "DataCollector.VERIFY_SSL_CERTIFICATES = False\n",
    "Transformer.VERIFY_SSL_CERTIFICATES = False\n",
    "control_hub = ControlHub(\n",
    "    cfg.getUrlSch(), username=cfg.getUser(), password=cfg.getPassword()\n",
    ")\n",
    "sdc = DataCollector(cfg.getUrlSdc(), control_hub=control_hub)\n",
    "sdc_authoring = control_hub.data_collectors.get(url=cfg.getUrlSdc())\n",
    "tx = control_hub.transformers.get(url=cfg.getUrlTx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to list the pipeline pattern stages properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ptx = control_hub.pipelines.get(name=\"tx_base_demo_sdk_factory_pat_01\")\n",
    "for p in dir(ptx):\n",
    "    if p.startswith(\"_\"):\n",
    "        continue\n",
    "    try:\n",
    "        v = eval(f\"ptx.{p}\")\n",
    "        print(\"property:{0}  value={1}\".format(p, v))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"*\" * 100)\n",
    "\n",
    "for p in dir(ptx.configuration):\n",
    "    if p.startswith(\"_\"):\n",
    "        continue\n",
    "    try:\n",
    "        v = eval(f\"ptx.configuration.{p}\")\n",
    "        print(\"property:{0}  value={1}\".format(p, v))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"*\" * 100)\n",
    "\n",
    "for stage in ptx.stages:\n",
    "    print(f\"----------------------------------------------------------\")\n",
    "    print(f\"----> {stage.stage_name}\")\n",
    "    print(f\"----------------------------------------------------------\")\n",
    "    for p in dir(stage):\n",
    "        if p.startswith(\"_\"):\n",
    "            continue\n",
    "        try:\n",
    "            v = eval(f\"stage.{p}\")\n",
    "            print(\"property:{0}  value={1}\".format(p, v))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember to customize file path(s), file name(s) based on your local setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /media/psf/Downloads/performance/dnb/col_width.txt\n",
    "11111111112111111111  33333333444444444455555555552022-01-01 12:00:00 AM 10.04\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.07\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 12:00:00 PM 10.06\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.045\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.55\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.89\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.34567\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 12:00:00 AM 10.04\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.07\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 12:00:00 PM 10.06\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.045\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.55\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.89\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.34567\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 12:00:00 AM 10.04\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.07\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 12:00:00 PM 10.06\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.045\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.55\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.89\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.34567\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 12:00:00 AM 10.04\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.07\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 12:00:00 PM 10.06\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.045\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.55\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.89\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.34567\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 12:00:00 AM 10.04\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.07\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 12:00:00 PM 10.06\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.045\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.55\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.89\n",
    "11111111112222222222  33333333444444444455555555552022-01-01 01:00:00 AM 10.34567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /media/psf/Downloads/performance/dnb/col_sep_duplicate_columns_0.csv\n",
    "col_0,col_1,col_2,col_3\n",
    "a,b,c,d\n",
    "a,b,c,d\n",
    "a,b,c,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /media/psf/Downloads/performance/dnb/col_sep_duplicate_columns_1.csv\n",
    "col_0,col_4,col_5,col_6,col_1\n",
    "a,b,c,d,e\n",
    "a,b,c,d,e\n",
    "a,b,c,d,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /media/psf/Downloads/performance/dnb/lookup_source_1.csv\n",
    "c,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Documentation for Spark SQL can be found here***: [Link to SparkSQL doc](https://spark.apache.org/docs/latest/api/sql/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Pattern # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = control_hub.get_pipeline_builder(fragment=False, transformer=tx)\n",
    "# --------------------------------- #\n",
    "\n",
    "s0 = pb.add_stage(\"File\", type=\"origin\")\n",
    "s0.directory_path = \"/media/psf/Downloads/performance/dnb\"\n",
    "s0.file_name_pattern = \"col_width.txt\"\n",
    "s0.data_format = \"TEXT\"\n",
    "s0.skip_offset_tracking = True\n",
    "\n",
    "# --------------------------------- #\n",
    "\n",
    "s1 = pb.add_stage(\"Spark SQL Expression\")\n",
    "s1.expressions = [\n",
    "    {\n",
    "        \"fieldName\": \"col_0\",\n",
    "        \"sqlExpression\": \"trim(substr(value,0,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"STRING\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_1\",\n",
    "        \"sqlExpression\": \"trim(substr(value,11,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"STRING\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_2\",\n",
    "        \"sqlExpression\": \"trim(substr(value,21,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"INTEGER\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_3\",\n",
    "        \"sqlExpression\": \"trim(substr(value,31,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"LONG\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_4\",\n",
    "        \"sqlExpression\": \"trim(substr(value,41,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"DOUBLE\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_5\",\n",
    "        \"sqlExpression\": \"to_date(trim(substr(value,51,22)),\\\"yyyy-MM-dd' 'hh:mm:ss' 'a\\\")\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"DATE\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_6\",\n",
    "        \"sqlExpression\": \"to_timestamp(trim(substr(value,51,22)),\\\"yyyy-MM-dd' 'hh:mm:ss' 'a\\\")\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"TIMESTAMP\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_7\",\n",
    "        \"sqlExpression\": \"trim(substr(value,73,9))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"DECIMAL\",\n",
    "        \"precision\": 8,\n",
    "        \"scale\": 5,\n",
    "    },\n",
    "]\n",
    "\n",
    "# --------------------------------- #\n",
    "\n",
    "s2 = pb.add_stage(\"Field Remover\")\n",
    "s2.fields = [\"value\"]\n",
    "\n",
    "# --------------------------------- #\n",
    "\n",
    "s3 = pb.add_stage(\"File\", type=\"destination\")\n",
    "s3.directory_path = \"/media/psf/Downloads/performance/data/generator_tmp/spark\"\n",
    "s3.data_format = \"JSON\"\n",
    "s3.write_mode = \"OVERWRITE\"\n",
    "# --------------------------------- #\n",
    "\n",
    "s0 >> s1 >> s2 >> s3\n",
    "\n",
    "# --------------------------------- #\n",
    "pipeline = pb.build(title=f\"tx_demo_sdk_factory_gen_00\")\n",
    "pipeline.add_label(\"sdk_factory/generated/based_pattern_00\")\n",
    "\n",
    "pipeline.configuration[\"executionMode\"] = \"BATCH\"\n",
    "pipeline.configuration[\"clusterConfig.clusterType\"] = \"LOCAL\"\n",
    "pipeline.configuration[\"clusterConfig.sparkMasterUrl\"] = \"local[4]\"\n",
    "pipeline.configuration[\"ludicrousMode\"] = True\n",
    "pipeline.configuration[\"ludicrousModeInputCount\"] = False\n",
    "\n",
    "pipeline.configuration[\"sparkConfigs\"] = [\n",
    "    {\"key\": \"spark.driver.memory\", \"value\": \"2G\"},\n",
    "    {\"key\": \"spark.driver.cores\", \"value\": \"1\"},\n",
    "    {\"key\": \"spark.executor.memory\", \"value\": \"2G\"},\n",
    "    {\"key\": \"spark.executor.cores\", \"value\": \"1\"},\n",
    "    {\"key\": \"spark.dynamicAllocation.enabled\", \"value\": \"true\"},\n",
    "    {\"key\": \"spark.shuffle.service.enabled\", \"value\": \"true\"},\n",
    "    {\"key\": \"spark.dynamicAllocation.minExecutors\", \"value\": \"1\"},\n",
    "]\n",
    "\n",
    "_ = control_hub.publish_pipeline(\n",
    "    pipeline, commit_message=f\"Initial commit by Pipeline-Factory\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Pattern # 2 - Version:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show how to leverage a pipeline splitter block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = control_hub.get_pipeline_builder(fragment=False, transformer=tx)\n",
    "# --------------------------------- #\n",
    "#  Add File origin stage\n",
    "# --------------------------------- #\n",
    "s0 = pb.add_stage(\"File\", type=\"origin\")\n",
    "s0.directory_path = \"/media/psf/Downloads/performance/dnb\"\n",
    "s0.file_name_pattern = \"col_width.txt\"\n",
    "s0.data_format = \"TEXT\"\n",
    "s0.skip_offset_tracking = True\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add Spark SQl to extract columns from input string\n",
    "# --------------------------------- #\n",
    "s1 = pb.add_stage(\"Spark SQL Expression\")\n",
    "s1.expressions = [\n",
    "    {\n",
    "        \"fieldName\": \"col_0\",\n",
    "        \"sqlExpression\": \"trim(substr(value,0,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"STRING\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_1\",\n",
    "        \"sqlExpression\": \"trim(substr(value,11,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"STRING\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_2\",\n",
    "        \"sqlExpression\": \"trim(substr(value,21,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"INTEGER\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_3\",\n",
    "        \"sqlExpression\": \"trim(substr(value,31,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"LONG\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_4\",\n",
    "        \"sqlExpression\": \"trim(substr(value,41,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"DOUBLE\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_5\",\n",
    "        \"sqlExpression\": \"to_date(trim(substr(value,51,22)),\\\"yyyy-MM-dd' 'hh:mm:ss' 'a\\\")\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"DATE\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_6\",\n",
    "        \"sqlExpression\": \"to_timestamp(trim(substr(value,51,22)),\\\"yyyy-MM-dd' 'hh:mm:ss' 'a\\\")\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"TIMESTAMP\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_7\",\n",
    "        \"sqlExpression\": \"trim(substr(value,73,9))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"DECIMAL\",\n",
    "        \"precision\": 8,\n",
    "        \"scale\": 5,\n",
    "    },\n",
    "]\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add Field remover to drop the input value column\n",
    "# --------------------------------- #\n",
    "s2 = pb.add_stage(\"Field Remover\")\n",
    "s2.fields = [\"value\"]\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add Spark a custom SQL Query\n",
    "# --------------------------------- #\n",
    "s3 = pb.add_stage(\"Spark SQL Query\")\n",
    "\n",
    "batch_id = 1\n",
    "order_by_column = \"col_0\"\n",
    "\n",
    "s3.query = f\"select a.*, {batch_id} as batch_id,ROW_NUMBER() OVER (ORDER BY {order_by_column}) AS row_id from $table a\"\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add a Stream Selector enabling pipeline lanes\n",
    "# --------------------------------- #\n",
    "s4 = pb.add_stage(\"Stream Selector\")\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add a Trash destination\n",
    "# --------------------------------- #\n",
    "s_trash_1 = pb.add_stage(\"Trash\")\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add a main data file destination\n",
    "# --------------------------------- #\n",
    "s5 = pb.add_stage(\"File\", type=\"destination\")\n",
    "s5.directory_path = \"/media/psf/Downloads/performance/data/generator_tmp/spark\"\n",
    "s5.data_format = \"JSON\"\n",
    "s5.write_mode = \"OVERWRITE\"\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add a Spark SQL query to generate a dataframe record count\n",
    "# --------------------------------- #\n",
    "s4_1 = pb.add_stage(\"Spark SQL Query\")\n",
    "s4_1.query = f\"select count(*) as record_count from $table a\"\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add a secondary (record_count) file destination\n",
    "# --------------------------------- #\n",
    "s4_2 = pb.add_stage(\"File\", type=\"destination\")\n",
    "s4_2.directory_path = \"/media/psf/Downloads/performance/data/generator_tmp/spark_count\"\n",
    "s4_2.data_format = \"JSON\"\n",
    "s4_2.write_mode = \"OVERWRITE\"\n",
    "\n",
    "# --------------------------------- #\n",
    "# Connect all the stages\n",
    "# --------------------------------- #\n",
    "s0 >> s1 >> s2 >> s3 >> s4\n",
    "s4 >> s5\n",
    "s4 >> s4_1 >> s4_2\n",
    "s4 >> s_trash_1\n",
    "\n",
    "# --------------------------------- #\n",
    "# Connect the conditions to output lanes\n",
    "# --------------------------------- #\n",
    "s4.condition = [\n",
    "    {\"outputLane\": s4.output_lanes[0], \"predicate\": \"0 == 0\"},\n",
    "    {\"outputLane\": s4.output_lanes[1], \"predicate\": \"0 == 0\"},\n",
    "    {\"outputLane\": s4.output_lanes[2], \"predicate\": \"default\"},\n",
    "]\n",
    "\n",
    "# --------------------------------- #\n",
    "#  Generate and publish the pipeline\n",
    "# --------------------------------- #\n",
    "pipeline = pb.build(title=f\"tx_demo_sdk_factory_gen_01_a\")\n",
    "pipeline.add_label(\"sdk_factory/generated/based_pattern_01\")\n",
    "\n",
    "pipeline.configuration[\"executionMode\"] = \"BATCH\"\n",
    "pipeline.configuration[\"clusterConfig.clusterType\"] = \"LOCAL\"\n",
    "pipeline.configuration[\"clusterConfig.sparkMasterUrl\"] = \"local[4]\"\n",
    "pipeline.configuration[\"ludicrousMode\"] = True\n",
    "pipeline.configuration[\"ludicrousModeInputCount\"] = False\n",
    "\n",
    "ptx.configuration[\"sparkConfigs\"] = [\n",
    "    {\"key\": \"spark.driver.memory\", \"value\": \"2G\"},\n",
    "    {\"key\": \"spark.driver.cores\", \"value\": \"1\"},\n",
    "    {\"key\": \"spark.executor.memory\", \"value\": \"2G\"},\n",
    "    {\"key\": \"spark.executor.cores\", \"value\": \"1\"},\n",
    "    {\"key\": \"spark.dynamicAllocation.enabled\", \"value\": \"true\"},\n",
    "    {\"key\": \"spark.shuffle.service.enabled\", \"value\": \"true\"},\n",
    "    {\"key\": \"spark.dynamicAllocation.minExecutors\", \"value\": \"1\"},\n",
    "]\n",
    "\n",
    "_ = control_hub.publish_pipeline(\n",
    "    pipeline, commit_message=f\"Initial commit by Pipeline-Factory\"\n",
    ")\n",
    "\n",
    "# --------------------------------- #\n",
    "#\n",
    "# --------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Pattern # 2 - Version:01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show how to connect one block (processor) to many downstream blocks using >> [b1,b2,..] syntax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = control_hub.get_pipeline_builder(fragment=False, transformer=tx)\n",
    "# --------------------------------- #\n",
    "#  Add File origin stage\n",
    "# --------------------------------- #\n",
    "s0 = pb.add_stage(\"File\", type=\"origin\")\n",
    "s0.directory_path = \"/media/psf/Downloads/performance/dnb\"\n",
    "s0.file_name_pattern = \"col_width.txt\"\n",
    "s0.data_format = \"TEXT\"\n",
    "s0.skip_offset_tracking = True\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add Spark SQl to extract columns from input string\n",
    "# --------------------------------- #\n",
    "s1 = pb.add_stage(\"Spark SQL Expression\")\n",
    "\n",
    "# ----> from meta-store\n",
    "s1.expressions = [\n",
    "    {\n",
    "        \"fieldName\": \"col_0\",\n",
    "        \"sqlExpression\": \"trim(substr(value,0,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"STRING\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_1\",\n",
    "        \"sqlExpression\": \"trim(substr(value,11,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"STRING\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_2\",\n",
    "        \"sqlExpression\": \"trim(substr(value,21,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"INTEGER\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_3\",\n",
    "        \"sqlExpression\": \"trim(substr(value,31,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"LONG\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_4\",\n",
    "        \"sqlExpression\": \"trim(substr(value,41,10))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"DOUBLE\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_5\",\n",
    "        \"sqlExpression\": \"to_date(trim(substr(value,51,22)),\\\"yyyy-MM-dd' 'hh:mm:ss' 'a\\\")\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"DATE\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_6\",\n",
    "        \"sqlExpression\": \"to_timestamp(trim(substr(value,51,22)),\\\"yyyy-MM-dd' 'hh:mm:ss' 'a\\\")\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"TIMESTAMP\",\n",
    "    },\n",
    "    {\n",
    "        \"fieldName\": \"col_7\",\n",
    "        \"sqlExpression\": \"trim(substr(value,73,9))\",\n",
    "        \"enableCastToType\": True,\n",
    "        \"castToType\": \"DECIMAL\",\n",
    "        \"precision\": 8,\n",
    "        \"scale\": 5,\n",
    "    },\n",
    "]\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add Field remover to drop the input value column\n",
    "# --------------------------------- #\n",
    "s2 = pb.add_stage(\"Field Remover\")\n",
    "s2.fields = [\"value\"]\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add Spark a custom SQL Query\n",
    "# --------------------------------- #\n",
    "s3 = pb.add_stage(\"Spark SQL Query\")\n",
    "\n",
    "# ----> from meta-store\n",
    "batch_id = 1\n",
    "order_by_column = \"col_0\"\n",
    "\n",
    "s3.query = f\"select a.*, {batch_id} as batch_id,ROW_NUMBER() OVER (ORDER BY {order_by_column}) AS row_id from $table a\"\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add a main data file destination\n",
    "# --------------------------------- #\n",
    "s5 = pb.add_stage(\"File\", type=\"destination\")\n",
    "s5.directory_path = \"/media/psf/Downloads/performance/data/generator_tmp/spark\"\n",
    "s5.data_format = \"JSON\"\n",
    "s5.write_mode = \"OVERWRITE\"\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add a Spark SQL query to generate a dataframe record count\n",
    "# --------------------------------- #\n",
    "s4_1 = pb.add_stage(\"Spark SQL Query\")\n",
    "s4_1.query = f\"select count(*) as record_count from $table a\"\n",
    "\n",
    "# --------------------------------- #\n",
    "# Add a secondary (record_count) file destination\n",
    "# --------------------------------- #\n",
    "s4_2 = pb.add_stage(\"File\", type=\"destination\")\n",
    "s4_2.directory_path = \"/media/psf/Downloads/performance/data/generator_tmp/spark_count\"\n",
    "s4_2.data_format = \"JSON\"\n",
    "s4_2.write_mode = \"OVERWRITE\"\n",
    "\n",
    "# --------------------------------- #\n",
    "# Connect all the stages\n",
    "# --------------------------------- #\n",
    "s0 >> s1 >> s2 >> s3 >> [s5, s4_1]\n",
    "s4_1 >> s4_2\n",
    "\n",
    "# --------------------------------- #\n",
    "#  Generate and publish the pipeline\n",
    "# --------------------------------- #\n",
    "pipeline = pb.build(title=f\"tx_demo_sdk_factory_gen_01_b\")\n",
    "pipeline.add_label(\"sdk_factory/generated/based_pattern_01\")\n",
    "\n",
    "pipeline.configuration[\"executionMode\"] = \"BATCH\"\n",
    "pipeline.configuration[\"clusterConfig.clusterType\"] = \"LOCAL\"\n",
    "pipeline.configuration[\"clusterConfig.sparkMasterUrl\"] = \"local[4]\"\n",
    "pipeline.configuration[\"ludicrousMode\"] = True\n",
    "pipeline.configuration[\"ludicrousModeInputCount\"] = False\n",
    "\n",
    "ptx.configuration[\"sparkConfigs\"] = [\n",
    "    {\"key\": \"spark.driver.memory\", \"value\": \"2G\"},\n",
    "    {\"key\": \"spark.driver.cores\", \"value\": \"1\"},\n",
    "    {\"key\": \"spark.executor.memory\", \"value\": \"2G\"},\n",
    "    {\"key\": \"spark.executor.cores\", \"value\": \"1\"},\n",
    "    {\"key\": \"spark.dynamicAllocation.enabled\", \"value\": \"true\"},\n",
    "    {\"key\": \"spark.shuffle.service.enabled\", \"value\": \"true\"},\n",
    "    {\"key\": \"spark.dynamicAllocation.minExecutors\", \"value\": \"1\"},\n",
    "]\n",
    "\n",
    "_ = control_hub.publish_pipeline(\n",
    "    pipeline, commit_message=f\"Initial commit by Pipeline-Factory\"\n",
    ")\n",
    "\n",
    "# --------------------------------- #\n",
    "#\n",
    "# --------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_pipelines = [\n",
    "    pipe for pipe in control_hub.pipelines.get_all() if pipe.name.startswith(\"tx_demo\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipeline in tx_pipelines:\n",
    "    job_builder = control_hub.get_job_builder()\n",
    "    jn = f\"job_{pipeline.name}\"\n",
    "\n",
    "    try:\n",
    "        _ = control_hub.jobs.get(job_name=jn)\n",
    "        control_hub.delete_job(jd)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    p = control_hub.pipelines.get(name=pipeline.name)\n",
    "\n",
    "    job = job_builder.build(\n",
    "        jn,\n",
    "        pipeline=p,\n",
    "        tags=[\"dnb_central_data_solution/tx/batch\"],\n",
    "    )\n",
    "    job.enable_time_series_analysis = True\n",
    "    job.enable_failover = False\n",
    "    job.description = jn\n",
    "    job.pipeline_force_stop_timeout = 30000\n",
    "    job.statistics_refresh_interval_in_millisecs = 60000\n",
    "    job.data_collector_labels = [\"eng-tx-000\"]\n",
    "    control_hub.add_job(job)\n",
    "    print(jn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Topologie "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not available yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topology_name = \"DNB_Central_Data_Solution\"\n",
    "# try:\n",
    "#     topology = control_hub.topologies.get(topology_name=topology_name)\n",
    "#     control_hub.delete_topology(topology)\n",
    "# except:\n",
    "#     pass\n",
    "# topo = control_hub.get_topology_builder()\n",
    "\n",
    "# # --------------------------------- #\n",
    "# #\n",
    "# # --------------------------------- #\n",
    "# tx_jobs =  [j for j in control_hub.jobs.get_all() if j.job_name.startswith(\"job_tx_demo_sdk_factory\")]\n",
    "# for tx_job in tx_jobs:\n",
    "#     print(tx_job.job_name)\n",
    "#     j = control_hub.jobs.get(job_name=tx_job.job_name)\n",
    "#     topo.add_job(j)\n",
    "\n",
    "# # --------------------------------- #\n",
    "# #\n",
    "# # --------------------------------- #\n",
    "# topology = topo.build(topology_name=topology_name)\n",
    "# _ = control_hub.publish_topology(topology)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28896b762d76329c10dab5730584f7da7bf435fe44680982515501198d4c3649"
  },
  "kernelspec": {
   "display_name": "snake38",
   "language": "python",
   "name": "snake38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
